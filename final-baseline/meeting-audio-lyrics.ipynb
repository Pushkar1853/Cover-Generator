{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install the required libraries\n!pip install transformers -q\n!pip install torchaudio -q\n!pip install nltk -q\n!pip install pydub -q ","metadata":{"execution":{"iopub.status.busy":"2023-03-14T18:44:59.142170Z","iopub.execute_input":"2023-03-14T18:44:59.142534Z","iopub.status.idle":"2023-03-14T18:45:43.587550Z","shell.execute_reply.started":"2023-03-14T18:44:59.142502Z","shell.execute_reply":"2023-03-14T18:45:43.586304Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelWithLMHead, AutoModelForCausalLM, AutoTokenizer\nfrom transformers import WhisperForConditionalGeneration, WhisperConfig, WhisperProcessor\nimport torchaudio\nimport nltk\nfrom pydub import AudioSegment\nimport re\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2023-03-14T18:46:15.323336Z","iopub.execute_input":"2023-03-14T18:46:15.323947Z","iopub.status.idle":"2023-03-14T18:46:25.287352Z","shell.execute_reply.started":"2023-03-14T18:46:15.323905Z","shell.execute_reply":"2023-03-14T18:46:25.286276Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model_name = \"openai/whisper-tiny.en\"\nmodel_config = WhisperConfig.from_pretrained(model_name)\nprocessor = WhisperProcessor.from_pretrained(model_name)\nasr_model = WhisperForConditionalGeneration.from_pretrained(model_name, config=model_config)\n\nasr_model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-03-14T18:49:37.577374Z","iopub.execute_input":"2023-03-14T18:49:37.577740Z","iopub.status.idle":"2023-03-14T18:49:39.828717Z","shell.execute_reply.started":"2023-03-14T18:49:37.577708Z","shell.execute_reply":"2023-03-14T18:49:39.827756Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"WhisperForConditionalGeneration(\n  (model): WhisperModel(\n    (encoder): WhisperEncoder(\n      (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n      (embed_positions): Embedding(1500, 384)\n      (layers): ModuleList(\n        (0): WhisperEncoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): WhisperEncoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): WhisperEncoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): WhisperEncoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): WhisperDecoder(\n      (embed_tokens): Embedding(51864, 384, padding_idx=50256)\n      (embed_positions): WhisperPositionalEmbedding(448, 384)\n      (layers): ModuleList(\n        (0): WhisperDecoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): WhisperDecoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): WhisperDecoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): WhisperDecoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (proj_out): Linear(in_features=384, out_features=51864, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# device = \"cuda:0\" if \"cuda:0\" else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2023-03-14T18:49:39.831513Z","iopub.execute_input":"2023-03-14T18:49:39.832153Z","iopub.status.idle":"2023-03-14T18:49:39.836474Z","shell.execute_reply.started":"2023-03-14T18:49:39.832114Z","shell.execute_reply":"2023-03-14T18:49:39.835466Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# asr_model = asr_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T18:49:39.837778Z","iopub.execute_input":"2023-03-14T18:49:39.838628Z","iopub.status.idle":"2023-03-14T18:49:39.846327Z","shell.execute_reply.started":"2023-03-14T18:49:39.838587Z","shell.execute_reply":"2023-03-14T18:49:39.845265Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\nsample = ds[0][\"audio\"]","metadata":{"execution":{"iopub.status.busy":"2023-03-14T18:49:40.005389Z","iopub.execute_input":"2023-03-14T18:49:40.005687Z","iopub.status.idle":"2023-03-14T18:49:41.527161Z","shell.execute_reply.started":"2023-03-14T18:49:40.005658Z","shell.execute_reply":"2023-03-14T18:49:41.526009Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \ntranscript = asr_model.generate(input_features)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T18:49:41.529560Z","iopub.execute_input":"2023-03-14T18:49:41.530188Z","iopub.status.idle":"2023-03-14T18:49:42.859565Z","shell.execute_reply.started":"2023-03-14T18:49:41.530145Z","shell.execute_reply":"2023-03-14T18:49:42.858504Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py:1278: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  UserWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted_ids = asr_model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T18:49:42.860991Z","iopub.execute_input":"2023-03-14T18:49:42.861455Z","iopub.status.idle":"2023-03-14T18:49:43.930604Z","shell.execute_reply.started":"2023-03-14T18:49:42.861414Z","shell.execute_reply":"2023-03-14T18:49:43.929587Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(transcription[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-14T18:49:43.932825Z","iopub.execute_input":"2023-03-14T18:49:43.933301Z","iopub.status.idle":"2023-03-14T18:49:43.939265Z","shell.execute_reply.started":"2023-03-14T18:49:43.933263Z","shell.execute_reply":"2023-03-14T18:49:43.938181Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\n","output_type":"stream"}]},{"cell_type":"code","source":"lyrics = transcription[0]","metadata":{},"execution_count":null,"outputs":[]}]}